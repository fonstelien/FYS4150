\documentclass[]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{color}
\usepackage{tabularx}
\usepackage{hyperref}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	%language=C++,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=false,
	breakatwhitespace=true,
	tabsize=2
}


\title{FYS4150 H20 - Project 4:\\The 2-Dimensional Ising Model and \\Monte Carlo Simulations}
\author{Olav Fønstelien}

\begin{document}
\maketitle

\begin{abstract}
%The abstract gives the reader a quick overview of what has been done and the most important results. Try to be to the point and state your main findings. It could be structured as follows 
% - Short introduction to topic and why its important 
% - Introduce a challenge or unresolved issue with the topic (that you will try to solve) 
% - What have you done to solve this 
% - Main Results 
% - The implications


Please visit my GitHub repository \url{https://github.com/fonstelien/FYS4150/tree/master/project4} for the source code developed for this report.

\end{abstract}

\section{Introduction} \label{intro}
%When you write the introduction you could focus on the following aspects
% - Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas
% - What I have done
% - The structure of the report, how it is organized etc

In this report we will study the Ising model and how this can be solved by statistical methods to show steady state conditions and phase transitions in two-dimensional magnetic systems. The model describes the binary state of each particle in a quadratic lattice. The state of each particle or \textit{spin} is either \textit{up} or \textit{down}, and the preferred state of each spin is to equal that of its four nearest neighbors in the lattice. The ground state is up, and one could maybe conclude that a lattice filled with ups makes for a dull model, but when applying an outer influence, like increasing the temperature, transitions away from the state of the neighbors become more likely. We would start to see scattered areas of downs in the lattice, and at one \textit{critical temperature} the complete breakdown of any order.

We will develop a simulation model for solving the Ising model in two dimensions. Even if an analytical solution to the two-dimensional model exists, the simulation model is easily extended beyond this. The Ising model can be used to solve problems in the social sciences, where the \textit{state} of one individual is heavily influenced by the state of its nearest neighbors. As such, our study in this report could serve as an introduction to a more general method of solving problems of this nature.

The model that we will develop uses the Monte Carlo method in combination with the Metropolis algorithm to simulate the distribution of ups and downs in an $L \times L$ lattice at a given temperature. We pick spins from the lattice by a stochastic process and decide whether to flip it or not -- \textit{yes} if the new state corresponds better to that of its neighbors; \textit{maybe} if not, to be decided by the Metropolis algorithm. We will see that this process, when repeated some tens to some hundred times over the lattice, forms a Markov process, with the lattice reaching its most likely state independently of its initial condition.

% MORE ABOUT THE REPORT STRUCTURE!


\section{Methods} \label{methods}
% - Describe the methods and algorithms
% - You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code
% - You should plug in some calculations to demonstrate your code, such as selected runs used to validate and verify your results. The latter is extremely important!! A reader needs to understand that your code reproduces selected benchmarks and reproduces previous results, either numerical and/or well-known closed form expressions.

% L=2. Find the analytical expression for the partition function and the corresponding expectations values for the energy E, the mean absolute value of the magnetic moment |M| (we will refer to this as the mean magnetization), the specific heat CV and the susceptibility χ as functions of T using periodic boundary conditions. These results will serve as benchmark calculations for our next steps. 

The magnetic energy of the spins in a lattice depends on their neighbors. The total energy $E$ of a lattice $\mathcal{L}$ is given by
\begin{equation} \label{eq:e-sum}
	E = -J \sum_{\langle kl \rangle} s_k s_l,
\end{equation}
where $\langle kl \rangle$ denotes all neighboring pairs of spins $s_k, s_l \in \{ -1, +1\}$ in the lattice. $J$ is the coupling constant and tells how strong the interrelation between the spins is. In this report we will assume that $J > 0$. The magnetic moment of the lattice is
\begin{equation} \label{eq:m-sum}
	\mathcal{M} = \sum_{k} s_k;
\end{equation}
a simple sum over all spins in the lattice.


If we now let the lattice $\mathcal{L}$ be quadratic with $L \times L$ spins, the likelihood of any spin configuration $\mathcal{L}_i$ at a temperature $T$ is given by the Boltzmann probability distribution as
\begin{equation} \label{eq:pi}
	P_i = \frac{e^{-\beta E_i}}{Z}, \quad \text{where} \quad \beta = \frac{1}{k_BT}.
\end{equation}
$E_i$ is $\mathcal{L}_i$'s magnetic energy in this configuration, as given by Equation (\ref{eq:e-sum}), and $Z$ is the partition function for the canonical ensemble (see \cite{fys4150-notes}). The partition function is defined as
\begin{equation}
	Z(\beta) = \sum_{i=1}^{M} e^{-\beta E_i},
\end{equation}
where $M = 2^{L^2}$ and $i = 1,2,...,M$ denotes each of the possible configurations $\mathcal{L}_i$ that $\mathcal{L}$ may have.

We see from Equations (\ref{eq:e-sum}), (\ref{eq:pi}) that if we let -1 denote a spin's \textit{down} state and +1 denote its \textit{up} state, the most likely state will be all ups, since this gives the lowest energy and hence highest probability, and at absolute zero temperature, this is how $\mathcal{L}$ will remain. 

However, increasing temperature will make other states more likely. Increased kinetic energy will cause individual spins to flip, giving the lattice a new state $\mathcal{L}_j$ with energy $E_j$. This process can be described by a Markov chain
\begin{equation} \label{eq:markov-chain}
	\mathbf{w}_{t+1} = \mathbf{W} \mathbf{w}_{t},
\end{equation}
where $\mathbf{w}_t = \{w_i\} = \{P_i\}$ denotes $\cal{L}$'s (discrete) probability distribution at time $t$. $\mathbf{W} = \{W_{i \rightarrow j}\}$ denotes the transition probabilities for all possible transitions $\mathcal{L}_i \rightarrow \mathcal{L}_j$, and is unknown. To overcome this, we will re-phrase $W_{i \rightarrow j}$ as a stochastic process, where the likelihood $T_{i \rightarrow j}$ of a spin to be a \textit{candidate} for the flip is equal for all, and the likelihood that we \textit{accept} this spin to flip is $A_{i \rightarrow j}$. That is;
\begin{equation}
	W_{i \rightarrow j} = T_{i \rightarrow j} A_{i \rightarrow j}.
\end{equation}

Transition the other way, $\mathcal{L}_j \rightarrow \mathcal{L}_i$, is also allowed. We let $W_{j \rightarrow i} = T_{j \rightarrow i} A_{j \rightarrow i}$ denote the reverse process, and by taking the right side of Equation (\ref{eq:markov-chain}) we get the so-called detailed balance;
\begin{equation}
	\frac{W_{i \rightarrow j}}{W_{j \rightarrow i}} 
	= \frac{T_{i \rightarrow j} A_{i \rightarrow j}}{T_{j \rightarrow i} A_{j \rightarrow i}} 
	= \frac{A_{i \rightarrow j}}{A_{j \rightarrow i}}
	= \frac{w_j}{w_i} = \frac{P_j}{P_i} = e^{-\beta \Delta E_{i \rightarrow j}}.
\end{equation}
Here, $\Delta E_{i \rightarrow j}$ denotes the change in energy between state $\mathcal{L}_i$ and $\mathcal{L}_j$, $\Delta E_{i \rightarrow j} = E_j - E_i$. 

We will use the Metropolis algorithm to establish the two unknowns $A_{i \rightarrow j}$ and $A_{j \rightarrow i}$. The Metropolis algorithm states that if $\Delta E_{i \rightarrow j} \le 0$, we flat out accept the transition, such that $A_{i \rightarrow j} = 1$. The likelihood that we accept the reverse then becomes $A_{j \rightarrow i} = e^{-\beta \Delta E_{j \rightarrow i}}$, which gives us the general form of the Metropolis algorithm;
\begin{equation} \label{eq:metropolis}
	A_{i \rightarrow j} = 
	\begin{cases}
	1 &\text{if} \quad \Delta E_{i \rightarrow j} \le 0 \\
	e^{-\beta \Delta E_{i \rightarrow j}} &\text{otherwise}
	\end{cases}.
\end{equation}

The decision whether to accept the flip on not is then made in another stochastic process. In each iteration we draw a new random  number $r_i \sim \mathcal{U}(0,1)$ and apply it to the Metropolis algorithm. The outcome is then summarized by the following rules:
\begin{equation} \label{eq:metropolis-accept-or-not}
\begin{aligned}
	r_i &< A_{i \rightarrow j} \rightarrow \text{accepted} \\
	r_i &\ge A_{i \rightarrow j} \rightarrow \text{not accepted} \\
\end{aligned} \quad.
\end{equation}

\vspace{5mm}

The transition probability $\mathbf{W}$ is a stochastic matrix, which means that it has a single largest eigenvalue $\lambda_{max} = \lambda_1 = 1$, and $\lambda_i < 1$ for the remaining \cite{fys4150-notes}. If we let $\mathbf{v}_i$ denote the corresponding eigenvectors, we can write the initial probability distribution $\mathbf{w}_{0}$ as
\begin{equation}
	\mathbf{w}_{0} = \sum_{i} \alpha_i \mathbf{v}_i.
\end{equation}
After the first iteration we get
\begin{equation}
	\mathbf{w}_{1} = \mathbf{W} \mathbf{w}_{0} = \sum_{i} \lambda_i \alpha_i \mathbf{v}_i,
\end{equation}
and if we continue to the $p$th iteration, $\mathbf{w}_{p} = \mathbf{W}^p \mathbf{w}_{0} = \sum_{i} \lambda_i^p \alpha_i \mathbf{v}_i$, it becomes obvious that we reach a limit where
\begin{equation}
	\mathbf{w}_{q} \approx \lambda_1^q \alpha_1 \mathbf{v}_1 = \alpha_1 \mathbf{v}_1.
\end{equation}
Since the sum of the probability distribution in $\mathbf{v}_1$ must be equal to 1, $\sum_{i} P_i = 1$, we must have $\alpha_1 = 1$. We then get that
\begin{equation} \label{eq:steady-state}
	\mathbf{w}_{t} = \mathbf{v}_1, \quad \text{for} \quad t \ge q,
\end{equation}
which means that after sufficient iterations, the probability distribution vector $\mathbf{w}_{t}$ becomes independent on the initial conditions $\mathbf{w}_{0}$. $\mathbf{w}_{t}$ will reach $\mathbf{v}_1$, which is the Boltzmann probability distribution for the defined partition function $Z(\beta)$ \cite{newman1999monte}, a quantity depending on temperature and the lattice itself. This means that if we allow ourselves enough time, the initial configuration of $\mathcal{L}$ can be selected freely since its probability distribution is bound to reach its most likely state from any initial configuration.

\vspace{5mm}

From Equation (\ref{eq:metropolis}) we see that the Metropolis algorithm lets us simulate the spin distribution of the lattice without knowing the partition function $Z(\beta)$. Evaluating this is not complicated, but it would require us to sum over all the possible configurations $\mathcal{L}_i$ of the lattice. The number of configurations is given by $M = 2^{L^2}$; a factor which very quickly becomes unwieldy. However, to derive any useful quantities for the lattice at a given temperature, it would still be necessary do sum over all configurations, unfortunately.

The expected magnetic energy and moment of the lattice are given by the equations
\begin{equation}
\begin{aligned}
	\mathbb{E}(E) &= \sum_{i=1}^{M} E_i P_i  = \frac{1}{Z} \sum_{i=1}^{M} E_i e^{-\beta E_i} \\
	\mathbb{E}(\mathcal{M}) &= \sum_{i=1}^{M} E_i P_i  = \frac{1}{Z} \sum_{i=1}^{M} \mathcal{M}_i e^{-\beta E_i}
\end{aligned},
\end{equation}
respectively. Further, the heat capacity $C_V$ and magnetic susceptibility $\chi$ are given by  
\begin{equation}
\begin{aligned}
	C_V &= \frac{\sigma^2_E}{k_B T^2} \\
	\chi &= \frac{\sigma^2_\mathcal{M}}{k_B T} \\
\end{aligned},
\end{equation}
where the variances $\sigma^2_E, \sigma^2_\mathcal{M}$ again are given by
\begin{equation}
\begin{aligned}
	\sigma^2_E &= \frac{1}{Z} \sum_{i=1}^{M} E^2_i e^{-\beta E_i} - \mathbb{E}^2(E) \\
	\sigma^2_\mathcal{M} &= \frac{1}{Z} \sum_{i=1}^{M} \mathcal{M}^2_i e^{-\beta E_i} - \mathbb{E}^2(\mathcal{M}) \\
\end{aligned}.
\end{equation}

We see that in addition to $Z$, summing over $M$ reappears in these equations. Our strategy will therefore be to use Monte Carlo simulation and approximate the expectation values and variances by \textit{sample means} and \textit{sample variances}. We will simulate the evolving configurations $\mathcal{L}_i$ of the lattice by the stochastic Markov process described in Equations (\ref{eq:markov-chain}) through (\ref{eq:metropolis}), then wait until the process reaches steady state (Equation (\ref{eq:steady-state})), and start sample the configuration energies and magnetic moments $E_i, \mathcal{M}_i$.

The general equations for sample mean $\bar{\mu}_X$ and variance $\bar{\sigma}^2_X$ by $N$ samples are given by
\begin{equation}
\begin{aligned}
	\bar{\mu}_X &= \frac{1}{N} \sum_{i=1}^{N} X_i \\
	\bar{\sigma}^2_X &= \frac{1}{N} \sum_{i=1}^{N} X^2_i - \bar{\mu}^2_X \\
\end{aligned},
\end{equation}
which have the limits $\bar{\mu}_X \rightarrow \mathbb{E}(X)$ when $N \rightarrow \infty$ \cite{fys-stk4155-notes}.

\vspace{5mm}

We will now study how to best implement our Monte Carlo simulation of the Ising model in a computer program. To get good approximations for the expectation values, we will need to do hundreds of thousands or even millions of spin flip tests on our lattice. That we develop efficient code is therefore a high priority.

Our lattice $\mathcal{L}$ has limited extension with $L \times L$ spins, and the first thing we need do is to define the boundary conditions. Since we wish to simulate as large lattices as possible, we will use so-called \textit{periodic} boundary conditions, where the spins on the lattice edges are imagined to neighbor on each other. See \cite{fys4150-notes}.

Next we randomly pick a candidate to flip among the spins in the lattice, say $s_k$. We use the Metropolis algorithm in Equation (\ref{eq:metropolis}) to decide whether the flip is accepted or not, based on the change $\Delta E_{i \rightarrow j}$ in the lattice's energy from state $\mathcal{L}_i$ to $\mathcal{L}_j$. The change in the lattice's energy is given by
\begin{equation} \label{eq:dE}
	\Delta E_{i \rightarrow j} = E_j - E_i = 2J s_k^{(i)} \sum_{\langle l \rangle} s_l,
\end{equation}
where $\langle l \rangle$ denotes $s_k$'s four neighbors and $s_k^{(i)}$ denotes its state in $\mathcal{L}_i$. We observe that the sum over $s_l$ can take only five different values, such that
\begin{equation}
	\Delta E_{i \rightarrow j} = \{-8J, -4J, 0, 4J, 8J \}.
\end{equation}

Consequently, the Metropolis algorithm acceptance criteria $A_{i \rightarrow j}$ in Equation (\ref{eq:metropolis}) can take only five corresponding values at a given temperature $T$;
\begin{equation} \label{eq:metropolis-acceptance}
	A_{i \rightarrow j} = \{ 1, 1, 1, e^{-4J \beta}, e^{-8J \beta} \}, \quad \text{where} \quad \beta = \frac{1}{k_BT}.
\end{equation}
Pre-calculating these values and keeping them in a lookup table before we start the Markov process thus saves the potentially expensive calculation of the exponential function during the Monte Carlo simulation.

Before we start the process we should also calculate the lattice's energy and magnetic moment in the initial condition, $E, \mathcal{M}$. Then, whenever a candidate is accepted, the change in energy and magnetic moment are accumulated. However, to avoid clogging the CPU pipeline with conditionals when we implement the Metropolis algorithm, we should use a facility that is available in many programming languages which is that boolean expressions return \lstinline|integer|s when they are evaluated; \lstinline|0| for a \lstinline|FALSE| expression and \lstinline|1| for a \lstinline|TRUE|. If we let $b$ denote the result of the Metropolis algorithm (accept/not accept), we get the update rules
\begin{equation}
\begin{aligned}
	s_k &\leftarrow s_k - 2bs_k \\
	\mathcal{M} &\leftarrow \mathcal{M} + 2bs_k^{(j)} \\
	E &\leftarrow E + 2bJ s_k^{(i)} \sum_{\langle l \rangle} s_l \\
\end{aligned}.
\end{equation}

\vspace{5mm}

Listing \ref{lst:ising} gives an outline of an algorithm for solving the Ising model in two dimensions. It is implemented as a Markov chain and runs a Monte Carlo simulation with the Metropolis algorithm as acceptance criteria. 

Note that we accumulate the absolute value \lstinline|Mabs| of the magnetic moment. The reason for this is that the model becomes unstable and may shift between positive and negative values close to the critical temperature. Note also that one Monte Carlo cycle is defined here as one sweep over the whole lattice, meaning that we do $L^2$ Metropolis spin flip tests per cycle. 

Before we start sampling magnetic energy and moment with \lstinline|Eacc|, \lstinline|E2acc|, \lstinline|Macc| and \lstinline|M2acc|, we must let the model reach its steady state (equilibration). This is handled by running a number of cycles defined by the \lstinline|equilibration_cycles| parameter. The number of cycles to run before equilibration depends on the size of the lattice, its initial and target temperatures, and also the sequence of numbers coming from the random number generator. It can only be determined by trial and error, but equilibration will generally be slower for a larger lattice, and for a larger difference in initial and target temperatures.

\begin{lstlisting}[caption={Solving the Ising model in two dimensions with Monte Carlo simulation and the Metropolis algorithm to approximate the mean values and variances of the magnetic energy and moment. Note that we accumulate the absolute value of the magnetic moment.},label={lst:ising},escapeinside={@}{@}] [!h]
	// Declarations
	Tinit : "initial temperature of the lattice"
	T : "target temperature of the lattice"	
	lattice : "LxL 2D array containing the spins"
	wij : "pre-calculated lookup table with Metropolis acceptance limits"
	equilibration_cycles : "non-sampled thermalizing cycles"
	monte_carlo_cycles : "sampled cycles"
	E, Mabs : "Energy and absolute value of magnetic moment in the lattice"
	Eacc, Macc : "Accumulated energy and magn. moment for mean calculation"
	E2acc, M2acc : "Acc. energy and magn. moment squared for variance calc."
	
	// Initializing
	init_lattice(lattice, Tinit)  // gives some initial state to spins
	init_metropolis(wij, T)  // @Equation (\ref{eq:metropolis-acceptance})@
	
	// Monte Carlo
	// Thermalizing...
	FOR i = 1...equilibration_cycles DO
		FOR j = 1...L*L DO
			sk : "randomly drawn spin from the lattice"
			dE : "delta-energy for sk acc. to @Equation (\ref{eq:dE})@"
			b : "1 or 0 based on metropolis algorithm with dE, wij (@Equation (\ref{eq:metropolis-accept-or-not})@)"
			sk @$\leftarrow$@ sk - 2*b
		END FOR
	END FOR
	
	// Sampling energy and magnetic moment
	"Initialize E, Mabs"
	"Set Eacc, Macc, E2acc, M2acc to 0"
	FOR i = 1...monte_carlo_cycles DO
		Eacc @$\leftarrow$@ Eacc + E
		E2acc @$\leftarrow$@ E2acc + E*E
		Macc @$\leftarrow$@ Macc + M
		M2acc @$\leftarrow$@ M2acc + M*M		
		FOR j = 1...L*L DO
			sk : "randomly drawn spin from the lattice"
			dE : "delta-energy for sk acc. to @Equation (\ref{eq:dE})@"
			b : "1 or 0 based on metropolis algorithm with dE, wij (@Equation (\ref{eq:metropolis-accept-or-not})@)"
			sk @$\leftarrow$@ sk - 2*b
			E @$\leftarrow$@ E + b*dE
			M @$\leftarrow$@ Mabs + 2*b*sk
		END FOR
	END FOR	
	
	// Expectation values
	Emean @$\leftarrow$@ Eacc / monte_carlo_cycles
	Mmean @$\leftarrow$@ Macc / monte_carlo_cycles

	// Variances
	Evar @$\leftarrow$@ E2acc / monte_carlo_cycles - Emean*Emean
	Mvar @$\leftarrow$@ M2acc / monte_carlo_cycles - Mmean*Mmean	
\end{lstlisting}




% Equations (13.6) and (13.7) of the lecture notes. Show that only five possible values of the energy differences ΔE are possible for the two-dimensional Ising model. Figure out how to encode efficiently the energy differences in the Boltzmann distribution. See the discussions in section 13.5 of the lecture notes. Why don't you need to calculate exp−ΔEβ each time you update the energy?


% Write now a code for the Ising model which computes the mean energy E, mean magnetization |M|, the specific heat CV and the susceptibility χ as functions of T using periodic boundary conditions



\clearpage
\section{Results} \label{results}
% - Present your results
% - Give a critical discussion of your work and place it in the correct context.
% - Relate your work to other calculations/studies
% - An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.
% - Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.

% for L=2 in the x and y directions. Compare your results with the expressions from a) for a temperature T=1.0 (in units of kT/J).How many Monte Carlo cycles do you need in order to achieve a good agreeement? 




\clearpage
\section{Discussion and Conclusion} \label{conclusion}
% - State your main findings and interpretations
% - Try as far as possible to present perspectives for future work
% - Try to discuss the pros and cons of the methods and possible improvements


%\clearpage
\bibliographystyle{unsrt}
\bibliography{project4.bib}
\end{document}
